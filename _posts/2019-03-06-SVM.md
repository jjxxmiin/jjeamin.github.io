---
layout: post
title:  "SVM"
summary: "Support vector machine"
date:   2019-03-06 22:00 -0400
categories: ml
---

# SVM
- Suport vector machine

2차원에 머무르는 것이 아니고 차원을 높여서 데이터를 나누는 과정을 말한다. 즉, 데이터를 분류하는 최적의 선을 찾는 것이다. Linear Regression지만 차원을 높여서 비선형 데이터의 분류도 가능하다.



![svm](/assets/img/post_img/ml/svm.PNG)



출처 : [https://bskyvision.com/163](https://bskyvision.com/163)


# 최적의 선(Decision Boundary)
- margin을 최대화 하는 선
- 가중치와 직교(90도) 해야 한다.

# Support vector
- 최적의선(혹은 초평면)에 가장 가까운 데이터

# margin
- 분류된 양쪽 데이터의 Support vector와 Decision Boundary간의 거리의 합을 말한다.
- Decision Boundary와 평행함

# Hinge loss
- 기존 Logistic Regression의 loss함수 cross-entropy와는 다르다.
- margin을 최대화 하기 위함
- 정답이 맞을 경우 : loss = 0
- 정답이 틀릴 경우 : loss = 증가



![loss](/assets/img/post_img/ml/hinge_loss.PNG)



- blue : hinge
- green : zero-one

# kernels
- 선형분류가 어려운 저차원 데이터를 고차원 공간으로 매핑
- 차원이 너무 높다면 많은 연산비용이 소모되어 사용이 어렵다 -> kernel trick 활용

# 초평면(Hyperplane)
- 차원을 높여서 비선형 데이터를 구별하면 선이라고 하기 보다는 평면에 가깝다.

---

# 참조
- [https://www.slideshare.net/freepsw/svm-77055058](https://www.slideshare.net/freepsw/svm-77055058)
- [https://nonmeyet.tistory.com/entry/R-SVM-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EC%84%A4%EB%AA%85](https://nonmeyet.tistory.com/entry/R-SVM-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EC%84%A4%EB%AA%85)
- [https://bskyvision.com/163](https://bskyvision.com/163)
